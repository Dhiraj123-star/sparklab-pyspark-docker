# ğŸ”¥ SparkLab

A production-grade, local PySpark development environment using Docker & Docker Compose â€” built for data engineers and developers on limited local resources.

---

## ğŸš€ Features

- ğŸ³ Containerized PySpark and Flask setup (Docker & Compose)
- ğŸ’¾ SQLite3 database integration
- ğŸŒ Flask API with endpoints for CSV data, database insertion, and retrieval
- ğŸ’¡ Optimized for local machines (8 GB RAM, 2 CPU cores)
- ğŸ§¼ Clean, isolated development environment
- ğŸ” Auto-restart on failure
- ğŸ” Production-grade structure
- ğŸ› ï¸ GitHub Actions for automated Docker image publishing

---

## âš™ï¸ Requirements

- ğŸ§© Docker
- ğŸ§© Docker Compose
- ğŸ§© GitHub account with Actions enabled
- ğŸ§© Docker Hub account for image publishing

---

## â–¶ï¸ Getting Started

1. ğŸ“¦ Build and run the project
   ```bash
   docker-compose up --build
   ```
2. ğŸŒ Access Flask API endpoints:
   - `GET /`: Health check
   - `GET /data`: Read data from CSV
   - `POST /insert?mode=[overwrite|append]`: Insert CSV data into SQLite3 database
   - `GET /db_data`: Retrieve data from SQLite3 database
   - `GET /data/<name>`: Retrieve a single record by name
   - `PUT /data/<name>`: Update age for a record by name
   - `DELETE /data/<name>`: Delete a record by name
3. âœ… View output logs in the container terminal
   ```bash
   docker logs sparklab_app
   ```
4. ğŸ”„ Make changes to your PySpark or Flask code and rerun

---

## ğŸ›‘ Stopping the App

- Gracefully shut down the environment
  ```bash
  docker-compose down
  ```
- Cleans up volumes and orphan containers automatically

---

## ğŸ› ï¸ GitHub Actions Setup

1. **Add Docker Hub Secrets**:
   - In your GitHub repository, go to `Settings > Secrets and variables > Actions`.
   - Add two secrets:
     - `DOCKERHUB_USERNAME`: Your Docker Hub username (e.g., `dhiraj918106`).
     - `DOCKERHUB_TOKEN`: Your Docker Hub access token (generate at `https://hub.docker.com/settings/security`).

2. **Verify Workflow**:
   - The `.github/workflows/ci.yml` file builds and pushes the Docker image to `dhiraj918106/sparklab-pyspark-docker:latest` on `main` branch pushes.
   - Check the workflow status in the `Actions` tab of your repository.

---

## ğŸ‘¨â€ğŸ’» Ideal For

- Data engineers building ETL prototypes
- Backend devs integrating Spark with Flask microservices
- Students exploring distributed computing with Spark and databases

---

Crafted for efficiency. Built for growth. Welcome to **SparkLab** ğŸ§ªâœ¨